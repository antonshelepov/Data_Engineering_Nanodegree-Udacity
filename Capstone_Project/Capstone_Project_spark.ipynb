{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project --- Spark\n",
    "This notebook shows the dimensions of immigration dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "This projects main goal is to deliver data about immigration to US for further analysis purposes of the marketing department of the company I work for as a data engineer. Stakeholders are interested not only in the airports and nearby cities the US visitors choose to travel to, but also about the demographics of this cities and temperature data.  \n",
    "\n",
    "#### I94 Immigration Data\n",
    "The dataset comes from the US National Tourism and Trade Office."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from pyspark.sql.functions import month\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_immi_data = '../../data/18-83510-I94-Data-2016/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Shape for apr:  columns: 28 rows: 3096313 <-- filename: ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\n",
      "--> Shape for sep:  columns: 28 rows: 3733786 <-- filename: ../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat\n",
      "--> Shape for nov:  columns: 28 rows: 2914926 <-- filename: ../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat\n",
      "--> Shape for mar:  columns: 28 rows: 3157072 <-- filename: ../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat\n",
      "--> Shape for jun:  columns: 34 rows: 3574989 <-- filename: ../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat\n",
      "--> Shape for aug:  columns: 28 rows: 4103570 <-- filename: ../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat\n",
      "--> Shape for may:  columns: 28 rows: 3444249 <-- filename: ../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat\n",
      "--> Shape for jan:  columns: 28 rows: 2847924 <-- filename: ../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat\n",
      "--> Shape for oct:  columns: 28 rows: 3649136 <-- filename: ../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat\n",
      "--> Shape for jul:  columns: 28 rows: 4265031 <-- filename: ../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat\n",
      "--> Shape for feb:  columns: 28 rows: 2570543 <-- filename: ../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat\n",
      "--> Shape for dec:  columns: 28 rows: 3432990 <-- filename: ../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "immi_files = glob.glob(dir_immi_data)\n",
    "\n",
    "for count,file in enumerate(immi_files):\n",
    "    month = re.search(r'.*\\/i94_(.*)16', file)\n",
    "    raw_df = spark.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "    df_immigration_shape = (raw_df.count(), len(raw_df.columns))\n",
    "    print(f'--> Shape for {month.group(1)}:  columns: {df_immigration_shape[1]} rows: {df_immigration_shape[0]} <-- filename: {file}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assesing the data\n",
    "Based on the numbers above (data immigrations files shape) it can stated, that files on average have 3 Mio. rows. Thereafter around 35 Mio. rows must be analyzed in order to extract valuable information\n",
    "\n",
    "#### Issue\n",
    "* jun file consists of 34 columns, whereas all over columns only have 28. Spark allows to only read data with same column number. In order to get the data extracted and transformed for each month a module has to be written, which removes unnecessary columns from jun data file\n",
    "* python module re is incompatible with spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal\n",
    "Manipulating data results very often in easy to follow visible changes to the data set. Here I will process data and compare shapes of individual raw and processed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visatype\n",
    "fname_visatype = 'input/visatype.csv'\n",
    "\n",
    "# country_code\n",
    "fname_country_code = 'i94_cit.csv'\n",
    "\n",
    "# airports (4 us_poe)\n",
    "fname_airports = 'airports.csv'\n",
    "\n",
    "dir_immi_data = '../../data/18-83510-I94-Data-2016/*'\n",
    "\n",
    "# visatype\n",
    "raw_visatype = pd.read_csv(fname_visatype,delimiter='|')\n",
    "\n",
    "# country_code\n",
    "raw_country_code = pd.read_csv(fname_country_code)\n",
    "\n",
    "# airports (4 us_poe)\n",
    "raw_airports = pd.read_csv(fname_airports)\n",
    "\n",
    "# visatype\n",
    "df_visatype = raw_visatype.copy()\n",
    "\n",
    "# country_code\n",
    "df_country_code = raw_country_code.copy()\n",
    "\n",
    "# airports (4 us_poe)\n",
    "df_airports = raw_airports.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to extract only valuable data from the files a number of manipulations must be made (see code block below):\n",
    "* matflag is based on matching arrival and departue records. Mismatched values will be filtered first\n",
    "* visatype will be matched with a table df_visatype. This comes in handy because by modifying the this table one can generate more or less data \n",
    "* column i94 contains information about way of transportation: 1 -> Air, 2 -> Sea, 3 -> Land. Stakeholders are only interested in analyzing airports\n",
    "* gender contains NaN values. These will be removed\n",
    "* another additional dataframe (country_code_list) will be used to filter data by Country of Citizenship & Country of Residence (i94cit, i94res)\n",
    "* the remaining rows will be filtered by using a list of US international Airports\n",
    "* immigration use SAS format in order to save dates. These must be transformed into conventional format\n",
    "* data can further be filtered by comparing arrival and departure dates. Logically, the latter cannot be smaller\n",
    "* month column will be generated in order to partition data more efficiently\n",
    "* columns that are not needed for further analysis will be removed\n",
    "* multiple columns will be set to type integer\n",
    "* for better communication multiple columns will be renamed (see Table above for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "immi_files = glob.glob(dir_immi_data)\n",
    "    \n",
    "def remove_extra(df):\n",
    "    \"\"\"This method loooks for pre-defined columns in a dataframe and removes them\n",
    "    \n",
    "    Params:\n",
    "        df: dataframe\n",
    "        \n",
    "    Returns: cleaned dataframe\n",
    "    \"\"\"\n",
    "    cols_2_remove = ['validres', 'delete_days', 'delete_mexl', 'delete_dup', 'delete_visa', 'delete_recdup']\n",
    "    df = df.drop(*cols_2_remove)\n",
    "    return df\n",
    "\n",
    "udf_to_datetime_sas = udf(lambda x: to_datetime(x), DateType())\n",
    "def to_datetime(x):\n",
    "    \"\"\"This method takes in a SAS coded date and converts it to a normal one\n",
    "    \n",
    "    Params:\n",
    "        x: SAS encoded date\n",
    "    \n",
    "    Returns: normal encoded date\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start = dt.datetime(1960, 1, 1).date()\n",
    "        return start + dt.timedelta(days=int(x))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def process_immi_data(spark, df_raw, df_visatype, df_country_code, df_airports):\n",
    "    \"\"\"This method cleanes immigration datasets\n",
    "    \n",
    "    Params:\n",
    "        spark: spark session\n",
    "        df_raw: dataset to manipulate\n",
    "        df_visatype: additional dataset for mapping purposes\n",
    "        df_country_code: additional dataset for mapping purposes\n",
    "        df_airports: additional dataset for mapping purposes\n",
    "    \n",
    "    Returns: df, cleaned immigration dataset\n",
    "    \"\"\"\n",
    "    # matflag is null\n",
    "    df_raw = df_raw.filter(df_raw.matflag.isNotNull())\n",
    "    # visatype GMT  \n",
    "    visatype_list = df_visatype.visatype.tolist()\n",
    "    df_raw = df_raw.filter( df_raw.visatype.isin(visatype_list) )\n",
    "    # i94mode other than 1 2 3\n",
    "    # 1: Air, 2: Sea, 3: Land\n",
    "    travel_mode = [1]\n",
    "    df_raw = df_raw.filter( df_raw.i94mode.isin(travel_mode) )\n",
    "    # gender is null\n",
    "    df_raw = df_raw.filter(df_raw.gender.isNotNull())\n",
    "    # Remove rows having invalid CoC & CoR\n",
    "    country_code_list = df_country_code.I94_country_code.astype('int').tolist()\n",
    "    df_raw = df_raw.filter( df_raw.i94cit.isin(country_code_list) )\n",
    "    df_raw = df_raw.filter( df_raw.i94res.isin(country_code_list) )\n",
    "    # filter only US international Airports\n",
    "    airports_us_list = df_airports.I94_port_code.tolist()\n",
    "    df_raw = df_raw.filter( df_raw.i94port.isin(airports_us_list) )\n",
    "    # Conversion of SAS encoded dates(arrdate & depdate)\n",
    "    df_raw = df_raw.withColumn(\"dt_arrival\", udf_to_datetime_sas(df_raw.arrdate))\n",
    "    df_raw = df_raw.withColumn(\"dt_departure\", udf_to_datetime_sas(df_raw.depdate))\n",
    "    # Departure date can't before Arrival date \n",
    "    df_raw = df_raw.filter(~(df_raw.dt_arrival > df_raw.dt_departure) | (df_raw.dt_departure.isNull()))\n",
    "    # Adding month which is used when saving file in parquet format partioning by month & landing state\n",
    "    df_raw = df_raw.withColumn(\"month\", month(\"dt_arrival\"))\n",
    "    # dropping columns\n",
    "    drop_cols = ['cicid', 'i94yr', 'i94mon', 'i94mode', 'i94visa', 'arrdate', 'depdate', 'count', 'dtadfile', 'entdepa', 'entdepd', 'entdepu', 'matflag', 'dtaddto', 'insnum', 'admnum']\n",
    "    df_raw = df_raw.drop(*drop_cols)\n",
    "    # change column type to integer\n",
    "    cols_2_integer = ['i94cit', 'i94res', 'i94bir', 'biryear']\n",
    "    for col in cols_2_integer:\n",
    "        df_raw = df_raw.na.fill(0, subset=[col])\n",
    "        df_raw = df_raw.withColumn(col, df_raw[col].cast(IntegerType()))\n",
    "    # Columns Rename\n",
    "    df_raw = (df_raw\n",
    "                .withColumnRenamed(\"i94cit\",  \"CoC\")\n",
    "                .withColumnRenamed(\"i94res\", \"CoR\")\n",
    "                .withColumnRenamed(\"i94port\", \"PoE\")\n",
    "                .withColumnRenamed(\"i94addr\", \"state_landing\")\n",
    "                .withColumnRenamed(\"i94bir\", \"age\")\n",
    "                .withColumnRenamed(\"biryear\", \"year_birth\")\n",
    "                .withColumnRenamed(\"airline\", \"airline_used\")\n",
    "                .withColumnRenamed(\"fltno\", \"num_flight\"))\n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing: 1 | 12 ---> ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\n",
      "--- Number of rows --- RAW DATA -[> 3096313 VS 1835551 <]-- PROCESSED DATA --- filename: ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\n",
      "--- Processing: 2 | 12 ---> ../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat\n",
      "--- Number of rows --- RAW DATA -[> 3733786 VS 2569913 <]-- PROCESSED DATA --- filename: ../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat\n",
      "--- Processing: 3 | 12 ---> ../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat\n",
      "--- Number of rows --- RAW DATA -[> 2914926 VS 1806168 <]-- PROCESSED DATA --- filename: ../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat\n",
      "--- Processing: 4 | 12 ---> ../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat\n",
      "--- Number of rows --- RAW DATA -[> 3157072 VS 1927600 <]-- PROCESSED DATA --- filename: ../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat\n",
      "--- Processing: 5 | 12 ---> ../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat\n",
      "--- Number of rows --- RAW DATA -[> 3574989 VS 2042963 <]-- PROCESSED DATA --- filename: ../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat\n",
      "--- Processing: 6 | 12 ---> ../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat\n",
      "--- Number of rows --- RAW DATA -[> 4103570 VS 2433852 <]-- PROCESSED DATA --- filename: ../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat\n",
      "--- Processing: 7 | 12 ---> ../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat\n",
      "--- Number of rows --- RAW DATA -[> 3444249 VS 2035945 <]-- PROCESSED DATA --- filename: ../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat\n",
      "--- Processing: 8 | 12 ---> ../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat\n",
      "--- Number of rows --- RAW DATA -[> 2847924 VS 1538098 <]-- PROCESSED DATA --- filename: ../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat\n",
      "--- Processing: 9 | 12 ---> ../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat\n",
      "--- Number of rows --- RAW DATA -[> 3649136 VS 2111336 <]-- PROCESSED DATA --- filename: ../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat\n",
      "--- Processing: 10 | 12 ---> ../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat\n",
      "--- Number of rows --- RAW DATA -[> 4265031 VS 2615442 <]-- PROCESSED DATA --- filename: ../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat\n",
      "--- Processing: 11 | 12 ---> ../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat\n",
      "--- Number of rows --- RAW DATA -[> 2570543 VS 1446752 <]-- PROCESSED DATA --- filename: ../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat\n",
      "--- Processing: 12 | 12 ---> ../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat\n",
      "--- Number of rows --- RAW DATA -[> 3432990 VS 2323712 <]-- PROCESSED DATA --- filename: ../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "for count,immi_file in enumerate(immi_files):\n",
    "    print(f'--- Processing: {count+1} | {len(immi_files)} ---> {immi_file}')\n",
    "    raw_df = spark.read.format('com.github.saurfang.sas.spark').load(immi_file)\n",
    "    df_processed = process_immi_data(spark, raw_df, df_visatype, df_country_code, df_airports)\n",
    "    raw_shape = (raw_df.count(), len(raw_df.columns))\n",
    "    processed_shape = (df_processed.count(), len(df_processed.columns))\n",
    "    print(f'--- Number of rows --- RAW DATA -[> {raw_shape[0]} VS {processed_shape[0]} <]-- PROCESSED DATA --- filename: {immi_file}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "After processing data and extracting only what is valuable for a particular usecase it can be stated that:\n",
    "* amount of data decreased significantly (on average each dataset was cut in half)\n",
    "* only keeping what is necessary increased the visibility for further analysis\n",
    "* transforming data is essential, as high compression formats keep data in a sort of a coded form (example: SAS date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
